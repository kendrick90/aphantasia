{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "toWe1IoH7X35"
   },
   "source": [
    "# IllusTrip: Text to Video 3D\n",
    "\n",
    "Part of [Aphantasia](https://github.com/eps696/aphantasia) suite, made by Vadim Epstein [[eps696](https://github.com/eps696)]  \n",
    "Based on [CLIP](https://github.com/openai/CLIP) + FFT/pixel ops from [Lucent](https://github.com/greentfrapp/lucent). \n",
    "3D part by [deKxi](https://twitter.com/deKxi), based on [AdaBins](https://github.com/shariqfarooq123/AdaBins) depth.  \n",
    "thanks to [Ryan Murdock](https://twitter.com/advadnoun), [Jonathan Fly](https://twitter.com/jonathanfly), [@eduwatch2](https://twitter.com/eduwatch2) for ideas.\n",
    "\n",
    "## Features \n",
    "* continuously processes **multiple sentences** (e.g. illustrating lyrics or poems)\n",
    "* makes **videos**, evolving with pan/zoom/rotate motion\n",
    "* works with [inverse FFT](https://github.com/greentfrapp/lucent/blob/master/lucent/optvis/param/spatial.py) representation of the image or **directly with RGB** pixels (no GANs involved)\n",
    "* generates massive detailed textures (a la deepdream), **unlimited resolution**\n",
    "* optional **depth** processing for 3D look\n",
    "* various CLIP models\n",
    "* can start/resume from an image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QytcEMSKBtN-"
   },
   "source": [
    "**Run the cell below after each session restart**\n",
    "\n",
    "Ensure that you're given Tesla T4/P4/P100 GPU, not K80!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title General setup\n",
    "\n",
    "import GPUtil as GPU\n",
    "from progress_bar import ProgressIPy as ProgressBar\n",
    "import depth\n",
    "import transforms\n",
    "from utils import slice_imgs, derivat, pad_up_to, slerp, checkout, sim_func, latent_anima\n",
    "from utils import basename, file_list, img_list, img_read, txt_clean, plot_text, old_torch\n",
    "from clip_fft import to_valid_rgb, fft_image, rfft2d_freqs, img2fft, pixel_image, un_rgb\n",
    "import lpips\n",
    "import kornia\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import clip\n",
    "import warnings\n",
    "import ipywidgets as ipy\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from IPython.display import HTML, Image, display, clear_output\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms as T\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from easydict import EasyDict as edict\n",
    "import shutil\n",
    "from base64 import b64encode\n",
    "import PIL\n",
    "import numpy as np\n",
    "import imageio\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import io\n",
    "from pathlib import Path\n",
    "import os\n",
    "!pip install ftfy == 5.8 transformers\n",
    "!pip install gputil ffpb\n",
    "!pip install imageio\n",
    "!pip install easydict\n",
    "!pip3 install torch == 1.10.0+cu113 torchvision == 0.11.1+cu113 torchaudio == =0.10.0+cu113 - f https: // download.pytorch.org/whl/cu113/torch_stable.html\n",
    "!pip install ipywidgets\n",
    "\n",
    "try:\n",
    "    !pip3 install googletrans == 3.1.0a0\n",
    "    from googletrans import Translator, constants\n",
    "    translator = Translator()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# !apt-get -qq install ffmpeg\n",
    "work_dir = Path.cwd()\n",
    "illustrip_dir = work_dir / 'illustrip'\n",
    "Path.mkdir(illustrip_dir, exist_ok=True)\n",
    "\n",
    "a = edict()\n",
    "\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "if os.name != \"nt\":\n",
    "    from google.colab import output, files\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "!pip install git+https: // github.com/openai/CLIP.git - -no-deps\n",
    "!pip install sentence_transformers\n",
    "!pip install kornia\n",
    "!pip install lpips\n",
    "!pip install PyWavelets == 1.1.1\n",
    "!pip install git+https: // github.com/fbcotter/pytorch_wavelets\n",
    "\n",
    "\n",
    "shutil.copy('mask.jpg', illustrip_dir)\n",
    "depth_mask_file = illustrip_dir / 'mask.jpg'\n",
    "clear_output()\n",
    "\n",
    "\n",
    "def save_img(img, fname=None):\n",
    "    img = np.array(img)[:, :, :]\n",
    "    img = np.transpose(img, (1, 2, 0))\n",
    "    img = np.clip(img*255, 0, 255).astype(np.uint8)\n",
    "    if fname is not None:\n",
    "        imageio.imsave(fname, np.array(img))\n",
    "        imageio.imsave('result.jpg', np.array(img))\n",
    "\n",
    "\n",
    "def makevid(seq_dir, size=None):\n",
    "    seq_dir = Path(seq_dir).as_posix()\n",
    "    char_len = len(basename(img_list(seq_dir)[0]))\n",
    "    out_sequence = seq_dir + '/%0{}d.jpg'.format(char_len)\n",
    "    out_video = seq_dir + '.mp4'\n",
    "    print('.. generating video ..')\n",
    "    !ffmpeg - y - v warning - i $out_sequence - crf 18 $out_video\n",
    "    data_url = \"data:video/mp4;base64,\" + \\\n",
    "        b64encode(open(out_video, 'rb').read()).decode()\n",
    "    wh = '' if size is None else 'width=%d height=%d' % (size, size)\n",
    "    # return \"\"\"<video %s controls><source src=\"%s\" type=\"video/mp4\"></video>\"\"\" % (wh, data_url)\n",
    "\n",
    "\n",
    "def makevidloop1(seq_dir, size=None):\n",
    "    seq_dir = Path(seq_dir).as_posix()\n",
    "    char_len = len(basename(img_list(seq_dir)[0]))\n",
    "    out_sequence = seq_dir + '/%0{}d.jpg'.format(char_len)\n",
    "    out_video = seq_dir + '.mp4'\n",
    "    print('.. generating video ..')\n",
    "    !ffmpeg - r 24 - i $out_sequence - filter_complex \"[0]reverse[r];[0][r]concat,setpts=N/24/TB\" - crf 18 - pix_fmt yuv420p $out_video\n",
    "    data_url = \"data:video/mp4;base64,\" + \\\n",
    "        b64encode(open(out_video, 'rb').read()).decode()\n",
    "    wh = '' if size is None else 'width=%d height=%d' % (size, size)\n",
    "    # return \"\"\"<video %s controls><source src=\"%s\" type=\"video/mp4\"></video>\"\"\" % (wh, data_url)\n",
    "\n",
    "\n",
    "def makevidloop2(seq_dir, size=None):\n",
    "    seq_dir = Path(seq_dir).as_posix()\n",
    "    char_len = len(basename(img_list(seq_dir)[0]))\n",
    "    out_sequence = seq_dir + '/%0{}d.jpg'.format(char_len)\n",
    "    out_video = seq_dir + '.mp4'\n",
    "    print('.. generating video ..')\n",
    "    !ffmpeg - r 24 - i $out_sequence - c: v h264_nvenc - filter_complex \"[0]reverse[r];[0][r]concat,setpts=N/24/TB\" - preset slow - rc constqp - qp 30 - pix_fmt yuv420p $out_video\n",
    "    wh = '' if size is None else 'width=%d height=%d' % (size, size)\n",
    "    # return \"\"\"<video %s controls><source src=\"%s\" type=\"video/mp4\"></video>\"\"\" % (wh, data_url)\n",
    "\n",
    "\n",
    "# Hardware check\n",
    "gpu = GPU.getGPUs()[0]  # XXX: only one GPU on Colab and isnâ€™t guaranteed\n",
    "print(\"(GPU RAM {0:.0f}MB | Free {1:.0f}MB)\".format(\n",
    "    gpu.memoryTotal, gpu.memoryFree))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "JUvpdy8BWGuM"
   },
   "outputs": [],
   "source": [
    "# @title Load inputs\n",
    "\n",
    "# @markdown **Content** (either type a text string, or upload a text file):\n",
    "# @param {type:\"string\"}\n",
    "content = \"pink brains connected to computers by a jumble of colorful wires.\"\n",
    "upload_texts = False  # @param {type:\"boolean\"}\n",
    "\n",
    "# @markdown **Style** (either type a text string, or upload a text file):\n",
    "style = \"\"  # @param {type:\"string\"}\n",
    "upload_styles = False  # @param {type:\"boolean\"}\n",
    "\n",
    "# @markdown For non-English languages use Google translation:\n",
    "translate = False  # @param {type:\"boolean\"}\n",
    "\n",
    "# @markdown Resume from the saved `.pt` snapshot, or from an image\n",
    "# @markdown (resolution settings below will be ignored in this case):\n",
    "resume = False  # @param {type:\"boolean\"}\n",
    "\n",
    "if upload_texts:\n",
    "    print('Upload main text file')\n",
    "    uploaded = files.upload()\n",
    "    text_file = list(uploaded)[0]\n",
    "    texts = list(uploaded.values())[0].decode().split('\\n')\n",
    "    texts = [tt.strip()\n",
    "             for tt in texts if len(tt.strip()) > 0 and tt[0] != '#']\n",
    "    print(' main text:', text_file, len(texts), 'lines')\n",
    "    workname = txt_clean(basename(text_file))\n",
    "else:\n",
    "    texts = [content]\n",
    "    workname = txt_clean(content)[:44]\n",
    "\n",
    "if upload_styles:\n",
    "    print('Upload styles text file')\n",
    "    uploaded = files.upload()\n",
    "    text_file = list(uploaded)[0]\n",
    "    styles = list(uploaded.values())[0].decode().split('\\n')\n",
    "    styles = [tt.strip()\n",
    "              for tt in styles if len(tt.strip()) > 0 and tt[0] != '#']\n",
    "    print(' styles:', text_file, len(styles), 'lines')\n",
    "else:\n",
    "    styles = [style]\n",
    "\n",
    "if resume:\n",
    "    print('Upload file to resume from')\n",
    "    resumed = files.upload()\n",
    "    resumed_filename = list(resumed)[0]\n",
    "    resumed_bytes = list(resumed.values())[0]\n",
    "\n",
    "assert len(texts) > 0 and len(texts[0]) > 0, 'No input text[s] found!'\n",
    "\n",
    "tempdir = illustrip_dir / workname\n",
    "counter = 1\n",
    "workname_num = workname\n",
    "while tempdir.exists():\n",
    "    counter += 1\n",
    "    workname_num = workname + \"_\" + str(counter)\n",
    "    tempdir = illustrip_dir / workname_num\n",
    "Path.mkdir(tempdir)\n",
    "print('main dir', tempdir)\n",
    "print(workname_num)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PQFGziYKtHSa"
   },
   "source": [
    "**`content`** (what to draw) is your primary input; **`style`** (how to draw) is optional, if you want to separate such descriptions.  \n",
    "If you load text file[s], the imagery will interpolate from line to line (ensure equal line counts for content and style lists, for their accordance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "64mlBCAYeOrB"
   },
   "outputs": [],
   "source": [
    "# @title Main settings\n",
    "\n",
    "sideX = 1920  # @param {type:\"integer\"} 1920\n",
    "sideY = 1080  # @param {type:\"integer\"} 1080\n",
    "steps = 100  # @param {type:\"integer\"}\n",
    "frame_step = 100  # @param {type:\"integer\"}\n",
    "# @markdown > Config\n",
    "method = 'RGB'  # @param ['FFT', 'RGB']\n",
    "# @param ['ViT-B/16', 'ViT-B/32', 'RN101', 'RN50x16', 'RN50x4', 'RN50']\n",
    "model = 'ViT-B/32'\n",
    "\n",
    "# Default settings\n",
    "if method == 'RGB':\n",
    "    align = 'overscan'\n",
    "    colors = 2\n",
    "    contrast = 1.2\n",
    "    sharpness = -1.\n",
    "    aug_noise = 0.\n",
    "    smooth = False\n",
    "else:\n",
    "    align = 'uniform'\n",
    "    colors = 1.8\n",
    "    contrast = 1.1\n",
    "    sharpness = 1.\n",
    "    aug_noise = 2.\n",
    "    smooth = True\n",
    "interpolate_topics = True\n",
    "style_power = 1.\n",
    "samples = 200\n",
    "save_step = 1\n",
    "learning_rate = 1.\n",
    "aug_transform = 'custom'\n",
    "similarity_function = 'cossim'\n",
    "macro = 0.4\n",
    "enforce = 0.\n",
    "expand = 0.\n",
    "zoom = 0.012\n",
    "shift = 10\n",
    "rotate = 0.8\n",
    "distort = 0.3\n",
    "animate_them = True\n",
    "sample_decrease = 1.\n",
    "DepthStrength = 0.\n",
    "\n",
    "print(' loading CLIP model..')\n",
    "model_clip, _ = clip.load(model, jit=old_torch())\n",
    "modsize = model_clip.visual.input_resolution\n",
    "xmem = {'ViT-B/16': 0.25, 'RN50': 0.5,\n",
    "        'RN50x4': 0.16, 'RN50x16': 0.06, 'RN101': 0.33}\n",
    "if model in xmem.keys():\n",
    "    sample_decrease *= xmem[model]\n",
    "\n",
    "clear_output()\n",
    "print(' using CLIP model', model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JIWNmmd5uuSn"
   },
   "source": [
    "**`FFT`** method uses inverse FFT representation of the image. It allows flexible motion, but is either blurry (if smoothed) or noisy (if not).  \n",
    "**`RGB`** method directly optimizes image pixels (without FFT parameterization). It's more clean and stable, when zooming in.  \n",
    "There are few choices for CLIP `model` (results do vary!). I prefer ViT-B/32 for consistency, next best bet is ViT-B/16.  \n",
    "\n",
    "**`steps`** defines the length of animation per text line (multiply it to the inputs line count to get total video duration in frames).  \n",
    "`frame_step` sets frequency of the changes in animation (how many frames between motion keypoints).  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f3Sj0fxmtw6K"
   },
   "source": [
    "## Other settings [optional]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "P88_xcpAIXlq"
   },
   "outputs": [],
   "source": [
    "# @title Run this cell to override settings, if needed\n",
    "# @markdown [to roll back defaults, run \"Main settings\" cell again]\n",
    "\n",
    "style_power = 1.  # @param {type:\"number\"}\n",
    "overscan = True  # @param {type:\"boolean\"}\n",
    "align = 'overscan' if overscan else 'uniform'\n",
    "interpolate_topics = True  # @param {type:\"boolean\"}\n",
    "\n",
    "# @markdown > Look\n",
    "colors = 2  # @param {type:\"number\"}\n",
    "contrast = 1.2  # @param {type:\"number\"}\n",
    "sharpness = 0.  # @param {type:\"number\"}\n",
    "\n",
    "# @markdown > Training\n",
    "samples = 500  # @param {type:\"integer\"}\n",
    "save_step = 12  # @param {type:\"integer\"}\n",
    "learning_rate = 2  # @param {type:\"number\"}\n",
    "\n",
    "# @markdown > Tricks\n",
    "aug_transform = 'custom'  # @param ['elastic', 'custom', 'none']\n",
    "aug_noise = 0.  # @param {type:\"number\"}\n",
    "macro = 0.4  # @param {type:\"number\"}\n",
    "enforce = 0.5  # @param {type:\"number\"}\n",
    "expand = 0.5  # @param {type:\"number\"}\n",
    "# @param ['cossim', 'spherical', 'mixed', 'angular', 'dot']\n",
    "similarity_function = 'cossim'\n",
    "\n",
    "# @markdown > Motion\n",
    "zoom = 0.012  # @param {type:\"number\"}\n",
    "shift = 10  # @param {type:\"number\"}\n",
    "rotate = 0.8  # @param {type:\"number\"}\n",
    "distort = 0.3  # @param {type:\"number\"}\n",
    "animate_them = True  # @param {type:\"boolean\"}\n",
    "smooth = True  # @param {type:\"boolean\"}\n",
    "if method == 'RGB':\n",
    "    smooth = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QYrJTb8xDm9C"
   },
   "source": [
    "`style_power` controls the strength of the style descriptions, comparing to the main input.  \n",
    "`overscan` provides better frame coverage (needed for RGB method).  \n",
    "`interpolate_topics` changes the subjects smoothly, otherwise they're switched by cut, making sharper transitions.  \n",
    "\n",
    "Decrease **`samples`** if you face OOM (it's the main RAM eater), or just to speed up the process (with the cost of quality).  \n",
    "`save_step` defines, how many optimization steps are taken between saved frames. Set it >1 for stronger image processing.   \n",
    "\n",
    "Experimental tricks:  \n",
    "`aug_transform` applies some augmentations, which quite radically change the output of this method (and slow down the process). Try yourself to see which is good for your case. `aug_noise` augmentation [FFT only!] seems to enhance optimization with transforms.  \n",
    "`macro` boosts bigger forms.  \n",
    "`enforce` adds more details by enforcing similarity between two parallel samples.  \n",
    "`expand` boosts diversity (up to irrelevant) by enforcing difference between prev/next samples.  \n",
    "\n",
    "Motion section:\n",
    "`shift` is in pixels, `rotate` in degrees. The values will be used as limits, if you mark `animate_them`.  \n",
    "\n",
    "`smooth` reduces blinking, but induces motion blur with subtle screen-fixed patterns (valid only for FFT method, disabled for RGB).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YdVubN0vb3TU"
   },
   "source": [
    "## Add 3D depth [optional]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "vl-rm1Nm03lK"
   },
   "outputs": [],
   "source": [
    "import gdown\n",
    "!pip install gdown\n",
    "# deKxi:: This whole cell contains most of whats needed,\n",
    "# with just a few changes to hook it up via frame_transform\n",
    "# (also glob_step now as global var)\n",
    "\n",
    "# I highly recommend performing the frame transformations and depth *after* saving,\n",
    "# (or just the depth warp if you prefer to keep the other affines as they are)\n",
    "# from my testing it reduces any noticeable stretching and allows the new areas\n",
    "# revealed from the changed perspective to be filled/detailed\n",
    "\n",
    "# pretrained models: Nyu is much better but Kitti is an option too\n",
    "depth_model = 'nyu'  # @ param [\"nyu\",\"kitti\"]\n",
    "DepthStrength = 0.01  # @param{type:\"number\"}\n",
    "MaskBlurAmt = 33  # @param{type:\"integer\"}\n",
    "save_depth = False  # @param{type:\"boolean\"}\n",
    "size = (sideY, sideX)\n",
    "\n",
    "# @markdown NB: depth computing may take up to ~3x more time. Read the comments inside for more info.\n",
    "\n",
    "# @markdown Courtesy of [deKxi](https://twitter.com/deKxi)\n",
    "\n",
    "if DepthStrength > 0:\n",
    "\n",
    "    if not os.path.exists(\"AdaBins_nyu.pt\"):\n",
    "        !gdown https: // drive.google.com/uc?id = 1lvyZZbC9NLcS8a__YPcUP7rDiIpbRpoF\n",
    "        if not os.path.exists('AdaBins_nyu.pt'):\n",
    "            !wget https: // www.dropbox.com/s/tayczpcydoco12s/AdaBins_nyu.pt\n",
    "    # if depth_model=='kitti' and not os.path.exists(os.path.join(workdir_depth, \"pretrained/AdaBins_kitti.pt\")):\n",
    "        # !gdown https://drive.google.com/uc?id=1HMgff-FV6qw1L0ywQZJ7ECa9VPq1bIoj\n",
    "\n",
    "    if save_depth:\n",
    "        depthdir = os.path.join(tempdir, 'depth')\n",
    "        os.makedirs(depthdir, exist_ok=True)\n",
    "        print('depth dir', depthdir)\n",
    "    else:\n",
    "        depthdir = None\n",
    "\n",
    "    depth_infer, depth_mask = depth.init_adabins(\n",
    "        model_path='AdaBins_nyu.pt', mask_path='mask.jpg', size=size)\n",
    "\n",
    "    def depth_transform(img_t, img_np, depth_infer, depth_mask, size, depthX=0, scale=1., shift=[0, 0], colors=1, depth_dir=None, save_num=0):\n",
    "\n",
    "        # d X/Y define the origin point of the depth warp, effectively a \"3D pan zoom\", [-1..1]\n",
    "        # plus = look ahead, minus = look aside\n",
    "        dX = 100. * shift[0] / size[1]\n",
    "        dY = 100. * shift[1] / size[0]\n",
    "        # dZ = movement direction: 1 away (zoom out), 0 towards (zoom in), 0.5 stay\n",
    "        dZ = 0.5 + 23. * (scale[0]-1)\n",
    "        # dZ += 0.5 * float(math.sin(((save_num % 70)/70) * math.pi * 2))\n",
    "\n",
    "        if img_np is None:\n",
    "            img2 = img_t.clone().detach()\n",
    "            par, imag, _ = pixel_image(img2.shape, resume=img2)\n",
    "            img2 = to_valid_rgb(imag, colors=colors)()\n",
    "            img2 = img2.detach().cpu().numpy()[0]\n",
    "            img2 = (np.transpose(img2, (1, 2, 0)))  # [h,w,c]\n",
    "            img2 = np.clip(img2*255, 0, 255).astype(np.uint8)\n",
    "            image_pil = T.ToPILImage()(img2)\n",
    "            del img2\n",
    "        else:\n",
    "            image_pil = T.ToPILImage()(img_np)\n",
    "\n",
    "        size2 = [s//2 for s in size]\n",
    "\n",
    "        img = depth.depthwarp(img_t, image_pil, depth_infer, depth_mask, size2, depthX, [\n",
    "                              dX, dY], dZ, rescale=0.5, clip_range=2, save_path=depth_dir, save_num=save_num)\n",
    "        return img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YZFuwNux8oEg"
   },
   "source": [
    "## Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "Nq0wA-wc-P-s"
   },
   "outputs": [],
   "source": [
    "# @title Generate\n",
    "\n",
    "# Delete memory from previous runs\n",
    "!nvidia-smi - caa\n",
    "for var in ['model_clip', 'perceptor', 'z']:\n",
    "    try:\n",
    "        del globals()[var]\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "try:\n",
    "    import gc\n",
    "    gc.collect()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    torch.cuda.empty_cache()\n",
    "except:\n",
    "    pass\n",
    "# End Clear Memory\n",
    "\n",
    "if aug_transform == 'elastic':\n",
    "    trform_f = transforms.transforms_elastic\n",
    "    sample_decrease *= 0.95\n",
    "elif aug_transform == 'custom':\n",
    "    trform_f = transforms.transforms_custom\n",
    "    sample_decrease *= 0.95\n",
    "else:\n",
    "    trform_f = transforms.normalize()\n",
    "\n",
    "if enforce != 0:\n",
    "    sample_decrease *= 0.5\n",
    "\n",
    "samples = int(samples * sample_decrease)\n",
    "print(' using %s method, %d samples' % (method, samples))\n",
    "\n",
    "if translate:\n",
    "    translator = Translator()\n",
    "\n",
    "\n",
    "def enc_text(txt):\n",
    "    if translate:\n",
    "        txt = translator.translate(txt, dest='en').text\n",
    "    emb = model_clip.encode_text(clip.tokenize(txt).cuda()[:77])\n",
    "    return emb.detach().clone()\n",
    "\n",
    "\n",
    "# Encode inputs\n",
    "count = 0  # max count of texts and styles\n",
    "key_txt_encs = [enc_text(txt) for txt in texts]\n",
    "count = max(count, len(key_txt_encs))\n",
    "key_styl_encs = [enc_text(style) for style in styles]\n",
    "count = max(count, len(key_styl_encs))\n",
    "assert count > 0, \"No inputs found!\"\n",
    "\n",
    "# !rm -rf $tempdir\n",
    "# os.makedirs(tempdir, exist_ok=True)\n",
    "\n",
    "# opt_steps = steps * save_step # for optimization\n",
    "glob_steps = count * steps  # saving\n",
    "if glob_steps == frame_step: frame_step = glob_steps // 2  # otherwise no motion\n",
    "\n",
    "outpic = ipy.Output()\n",
    "outpic\n",
    "\n",
    "if method == 'RGB':\n",
    "\n",
    "    if resume:\n",
    "        img_in = imageio.imread(resumed_bytes) / 255.\n",
    "        params_tmp = torch.Tensor(img_in).permute(\n",
    "            2, 0, 1).unsqueeze(0).float().cuda()\n",
    "        params_tmp = un_rgb(params_tmp, colors=1.)\n",
    "        sideY, sideX = img_in.shape[0], img_in.shape[1]\n",
    "    else:\n",
    "        params_tmp = torch.randn(1, 3, sideY, sideX).cuda()  # * 0.01\n",
    "\n",
    "else:  # FFT\n",
    "\n",
    "if resume:\n",
    "    if os.path.splitext(resumed_filename)[1].lower()[1:] in ['jpg', 'png', 'tif', 'bmp']:\n",
    "      img_in = imageio.imread(resumed_bytes)\n",
    "      params_tmp = img2fft(img_in, 1.5, 1.) * 2.\n",
    "    else:\n",
    "      params_tmp = torch.load(io.BytesIO(resumed_bytes))\n",
    "      if isinstance(params_tmp, list): params_tmp = params_tmp[0]\n",
    "    params_tmp = params_tmp.cuda()\n",
    "    sideY, sideX = params_tmp.shape[2], (params_tmp.shape[3]-1)*2\n",
    "  else:\n",
    "    params_shape = [1, 3, sideY, sideX//2+1, 2]\n",
    "    params_tmp = torch.randn(*params_shape).cuda() * 0.01\n",
    "  \n",
    "params_tmp = params_tmp.detach()\n",
    "# function() = torch.transformation(linear)\n",
    "\n",
    "# animation controls\n",
    "if animate_them:\n",
    "  if method == 'RGB':\n",
    "    m_scale = latent_anima([1], glob_steps, frame_step, uniform=True, cubic=True, start_lat=[-0.3])\n",
    "    m_scale = 1 + (m_scale + 0.3) * zoom # only zoom in\n",
    "  else:\n",
    "    m_scale = latent_anima([1], glob_steps, frame_step, uniform=True, cubic=True, start_lat=[0.6])\n",
    "    m_scale = 1 - (m_scale-0.6) * zoom # ping pong\n",
    "  m_shift = latent_anima([2], glob_steps, frame_step, uniform=True, cubic=True, start_lat=[0.5,0.5])\n",
    "  m_angle = latent_anima([1], glob_steps, frame_step, uniform=True, cubic=True, start_lat=[0.5])\n",
    "  m_shear = latent_anima([1], glob_steps, frame_step, uniform=True, cubic=True, start_lat=[0.5])\n",
    "  m_shift = (m_shift-0.5) * shift   * abs(m_scale-1.) / zoom\n",
    "  m_angle = (m_angle-0.5) * rotate  * abs(m_scale-1.) / zoom\n",
    "  m_shear = (m_shear-0.5) * distort * abs(m_scale-1.) / zoom\n",
    "\n",
    "def get_encs(encs, num):\n",
    "  cnt = len(encs)\n",
    "  if cnt == 0: return []\n",
    "  enc_1 = encs[min(num,   cnt-1)]\n",
    "  enc_2 = encs[min(num+1, cnt-1)]\n",
    "  return slerp(enc_1, enc_2, steps)\n",
    "\n",
    "def frame_transform(img, size, angle, shift, scale, shear):\n",
    "  if old_torch(): # 1.7.1\n",
    "    img = T.functional.affine(img, angle, shift, scale, shear, fillcolor=0, resample=PIL.Image.BILINEAR)\n",
    "    img = T.functional.center_crop(img, size)\n",
    "    img = pad_up_to(img, size)\n",
    "  else: # 1.8+\n",
    "    img = T.functional.affine(img, angle, shift, scale, shear, fill=0, interpolation=T.InterpolationMode.BILINEAR)\n",
    "    img = T.functional.center_crop(img, size) # on 1.8+ also pads\n",
    "  return img\n",
    "\n",
    "global img_np\n",
    "img_np = None\n",
    "prev_enc = 0\n",
    "stop_on_next_loop = False  # Make sure GPU memory doesn't get corrupted from cancelling the run mid-way through, allow a full frame to complete\n",
    "def process(num):\n",
    "  if stop_on_next_loop:\n",
    "    break\n",
    "  global params_tmp, img_np, opt_state, params, image_f, optimizer, pbar\n",
    "\n",
    "  if interpolate_topics:\n",
    "    txt_encs  = get_encs(key_txt_encs,  num)\n",
    "    styl_encs = get_encs(key_styl_encs, num)\n",
    "  else:\n",
    "    txt_encs  = [key_txt_encs[min(num,  len(key_txt_encs)-1)][0]]  * steps if len(key_txt_encs)  > 0 else []\n",
    "    styl_encs = [key_styl_encs[min(num, len(key_styl_encs)-1)][0]] * steps if len(key_styl_encs) > 0 else []\n",
    "\n",
    "  if len(texts)  > 0: print(' ref text: ',  texts[min(num, len(texts)-1)][:80])\n",
    "  if len(styles) > 0: print(' ref style: ', styles[min(num, len(styles)-1)][:80])\n",
    "\n",
    "  for ii in range(steps):\n",
    "    glob_step = num * steps + ii # saving/transforming\n",
    "\n",
    "    # animation: transform frame, reload params\n",
    "\n",
    "    h, w = sideY, sideX\n",
    "    \n",
    "    # transform frame for motion\n",
    "    scale =       m_scale[glob_step]    if animate_them else 1-zoom\n",
    "    trans = tuple(m_shift[glob_step])   if animate_them else [0, shift]\n",
    "    angle =       m_angle[glob_step][0] if animate_them else rotate\n",
    "    shear =       m_shear[glob_step][0] if animate_them else distort\n",
    "\n",
    "    if method == 'RGB':\n",
    "      if DepthStrength > 0:\n",
    "        params_tmp = depth_transform(params_tmp, img_np, depth_infer, depth_mask, size, DepthStrength, scale, trans, colors, depthdir, glob_step)\n",
    "      params_tmp = frame_transform(params_tmp, (h,w), angle, trans, scale, shear)\n",
    "      params, image_f, _ = pixel_image([1,3,h,w], resume=params_tmp)\n",
    "      img_tmp = None\n",
    "\n",
    "    else: # FFT\n",
    "      if old_torch(): # 1.7.1\n",
    "        img_tmp = torch.irfft(params_tmp, 2, normalized=True, signal_sizes=(h,w))\n",
    "        if DepthStrength > 0:\n",
    "          img_tmp = depth_transform(img_tmp, img_np, depth_infer, depth_mask, size, DepthStrength, scale, trans, colors, depthdir, glob_step)\n",
    "        img_tmp = frame_transform(img_tmp, (h,w), angle, trans, scale, shear)\n",
    "        params_tmp = torch.rfft(img_tmp, 2, normalized=True)\n",
    "      else: # 1.8+\n",
    "        if type(params_tmp) is not torch.complex64:\n",
    "          params_tmp = torch.view_as_complex(params_tmp)\n",
    "        img_tmp = torch.fft.irfftn(params_tmp, s=(h,w), norm='ortho')\n",
    "        if DepthStrength > 0:\n",
    "          img_tmp = depth_transform(img_tmp, img_np, depth_infer, depth_mask, size, DepthStrength, scale, trans, colors, depthdir, glob_step)\n",
    "        img_tmp = frame_transform(img_tmp, (h,w), angle, trans, scale, shear)\n",
    "        params_tmp = torch.fft.rfftn(img_tmp, s=[h,w], dim=[2,3], norm='ortho')\n",
    "        params_tmp = torch.view_as_real(params_tmp)\n",
    "\n",
    "      params, image_f, _ = fft_image([1,3,h,w], resume=params_tmp, sd=1.)\n",
    "\n",
    "    image_f = to_valid_rgb(image_f, colors=colors)\n",
    "    del img_tmp\n",
    "    optimizer = torch.optim.Adam(params, learning_rate)\n",
    "    # optimizer = torch.optim.AdamW(params, learning_rate, weight_decay=0.01, amsgrad=True)\n",
    "    if smooth is True and num + ii > 0:\n",
    "      optimizer.load_state_dict(opt_state)\n",
    "\n",
    "    # get encoded inputs\n",
    "    txt_enc  = txt_encs[ii % len(txt_encs)].unsqueeze(0)   if len(txt_encs)  > 0 else None\n",
    "    styl_enc = styl_encs[ii % len(styl_encs)].unsqueeze(0) if len(styl_encs) > 0 else None\n",
    "          \n",
    "    # optimization\n",
    "\n",
    "    for ss in range(save_step):\n",
    "      loss = 0\n",
    "\n",
    "      noise = aug_noise * (torch.rand(1, 1, *params[0].shape[2:4], 1)-0.5).cuda() if aug_noise > 0 else 0.\n",
    "      img_out = image_f(noise)\n",
    "      img_sliced = slice_imgs([img_out], samples, modsize, trform_f, align, macro)[0]\n",
    "      out_enc = model_clip.encode_image(img_sliced)\n",
    "\n",
    "      if method == 'RGB': # empirical hack\n",
    "        loss += 1.5 * abs(img_out.mean((2,3)) - 0.45).mean() # fix brightness\n",
    "        loss += 1.5 * abs(img_out.std((2,3)) - 0.17).sum() # fix contrast\n",
    "\n",
    "      if txt_enc is not None:\n",
    "        loss -= sim_func(txt_enc, out_enc, similarity_function)\n",
    "      if styl_enc is not None:\n",
    "        loss -= style_power * sim_func(styl_enc, out_enc, similarity_function)\n",
    "      if sharpness != 0: # mode = scharr|sobel|naive\n",
    "        loss -= sharpness * derivat(img_out, mode='naive')\n",
    "        # loss -= sharpness * derivat(img_sliced, mode='scharr')\n",
    "      if enforce != 0:\n",
    "        img_sliced = slice_imgs([image_f(noise)], samples, modsize, trform_f, align, macro)[0]\n",
    "        out_enc2 = model_clip.encode_image(img_sliced)\n",
    "        loss -= enforce * sim_func(out_enc, out_enc2, similarity_function)\n",
    "        del out_enc2; torch.cuda.empty_cache()\n",
    "      if expand > 0:\n",
    "        global prev_enc\n",
    "        if ii > 0:\n",
    "          loss += expand * sim_func(prev_enc, out_enc, similarity_function)\n",
    "        prev_enc = out_enc.detach().clone()\n",
    "      del img_out, img_sliced, out_enc; torch.cuda.empty_cache()\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "    \n",
    "    # save params & frame\n",
    "\n",
    "    params_tmp = params[0].detach().clone()\n",
    "    if smooth is True:\n",
    "      opt_state = optimizer.state_dict()\n",
    "\n",
    "    with torch.no_grad():\n",
    "      img_t = image_f(contrast=contrast)[0].permute(1,2,0)\n",
    "      img_np = torch.clip(img_t*255, 0, 255).cpu().numpy().astype(np.uint8)\n",
    "    imageio.imsave(os.path.join(tempdir, '%05d.jpg' % glob_step), img_np, quality=95)\n",
    "    shutil.copy(os.path.join(tempdir, '%05d.jpg' % glob_step), 'result.jpg')\n",
    "    outpic.clear_output()\n",
    "    with outpic:\n",
    "      display(Image('result.jpg'))\n",
    "    del img_t\n",
    "    pbar.upd()\n",
    "\n",
    "  params_tmp = params[0].detach().clone()\n",
    "\n",
    "outpic = ipy.Output()\n",
    "outpic\n",
    "\n",
    "pbar = ProgressBar(glob_steps)\n",
    "for i in range(count):\n",
    "  process(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# makevid(tempdir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# makevidloop1(tempdir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# makevidloop2(tempdir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.0+cu113\n",
      "GPU 0: Quadro RTX 5000 with Max-Q Design (UUID: GPU-2717452d-244f-af09-1aac-9641f82786b2)\n",
      "Tue Dec  7 19:21:10 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 496.49       Driver Version: 496.49       CUDA Version: 11.5     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Quadro RTX 5000... WDDM  | 00000000:01:00.0  On |                  N/A |\n",
      "| N/A   67C    P0    76W /  N/A |  16190MiB / 16384MiB |     55%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1492    C+G   Insufficient Permissions        N/A      |\n",
      "|    0   N/A  N/A      7636    C+G   ...me\\Application\\chrome.exe    N/A      |\n",
      "|    0   N/A  N/A      8908    C+G   C:\\Windows\\explorer.exe         N/A      |\n",
      "|    0   N/A  N/A     11720    C+G   ...2txyewy\\TextInputHost.exe    N/A      |\n",
      "|    0   N/A  N/A     13172    C+G   ... Host\\Razer Synapse 3.exe    N/A      |\n",
      "|    0   N/A  N/A     17248      C   ...vs\\illustrip3d\\python.exe    N/A      |\n",
      "|    0   N/A  N/A     20284    C+G   ...bbwe\\Microsoft.Photos.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# print(torch.__version__)\n",
    "# torch.cuda.is_available()\n",
    "# torch.cuda.empty_cache()\n",
    "# !nvidia-smi -L\n",
    "# !nvidia-smi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Dec  7 19:44:02 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 496.49       Driver Version: 496.49       CUDA Version: 11.5     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Quadro RTX 5000... WDDM  | 00000000:01:00.0  On |                  N/A |\n",
      "| N/A   68C    P0    85W /  N/A |  16044MiB / 16384MiB |     83%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1492    C+G   Insufficient Permissions        N/A      |\n",
      "|    0   N/A  N/A      7636    C+G   ...me\\Application\\chrome.exe    N/A      |\n",
      "|    0   N/A  N/A      8908    C+G   C:\\Windows\\explorer.exe         N/A      |\n",
      "|    0   N/A  N/A     11720    C+G   ...2txyewy\\TextInputHost.exe    N/A      |\n",
      "|    0   N/A  N/A     13172    C+G   ... Host\\Razer Synapse 3.exe    N/A      |\n",
      "|    0   N/A  N/A     17248      C   ...vs\\illustrip3d\\python.exe    N/A      |\n",
      "|    0   N/A  N/A     20284    C+G   ...bbwe\\Microsoft.Photos.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to clear Accounted PIDs for GPU 00000000:01:00.0: Insufficient Permissions\n",
      "Terminating early due to previous errors.\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi - caa\n",
    "for var in ['model_clip']:\n",
    "    try:\n",
    "        del globals()[var]\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "try:\n",
    "    import gc\n",
    "    gc.collect()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    torch.cuda.empty_cache()\n",
    "except:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "f3Sj0fxmtw6K",
    "YdVubN0vb3TU"
   ],
   "name": "IllusTrip3D.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "interpreter": {
   "hash": "3e50bcc284b88bea6b4f3e2c7446bd7a1aa9b57b6d8641d26e6bf91bbc7ec277"
  },
  "kernelspec": {
   "display_name": "Python [conda env:illustrip3d]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
